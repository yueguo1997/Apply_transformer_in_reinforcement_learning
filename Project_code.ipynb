{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvUWpfbmjwcc5Dl5oWH132",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yueguo1997/Apply_transformer_in_reinforcement_learning/blob/main/Project_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c34HqATGhuPH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerLayer(layers.Layer):\n",
        "    def __init__(self, num_heads, hidden_size, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.multi_head_attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=hidden_size)\n",
        "        self.norm1 = layers.LayerNormalization()\n",
        "        self.dropout1 = layers.Dropout(dropout_rate)\n",
        "        self.feed_forward = tf.keras.Sequential([\n",
        "            layers.Dense(hidden_size * 4, activation='relu'),\n",
        "            layers.Dense(hidden_size)\n",
        "        ])\n",
        "        self.norm2 = layers.LayerNormalization()\n",
        "        self.dropout2 = layers.Dropout(dropout_rate)\n",
        "        \n",
        "    def call(self, inputs, training=False):\n",
        "        attention_output = self.multi_head_attention(inputs, inputs)\n",
        "        attention_output = self.dropout1(attention_output, training=training)\n",
        "        attention_output = self.norm1(inputs + attention_output)\n",
        "        feed_forward_output = self.feed_forward(attention_output)\n",
        "        feed_forward_output = self.dropout2(feed_forward_output, training=training)\n",
        "        return self.norm2(attention_output + feed_forward_output)\n",
        "\n",
        "class TransformerNet(tf.keras.Model):\n",
        "    def __init__(self, num_layers, num_heads, hidden_size, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = layers.Dense(hidden_size)\n",
        "        self.layers = [TransformerLayer(num_heads, hidden_size, dropout_rate)\n",
        "                       for _ in range(num_layers)]\n",
        "        self.flatten = layers.Flatten()\n",
        "        self.policy = layers.Dense(2, activation='softmax')\n",
        "        self.value = layers.Dense(1)\n",
        "        \n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.embedding(inputs)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, training=training)\n",
        "        x = self.flatten(x)\n",
        "        policy = self.policy(x)\n",
        "        value = self.value(x)\n",
        "        return policy, value\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class PPOAgent:\n",
        "    def __init__(self, env, net, clip_ratio=0.2, gamma=0.99, entropy_coefficent=0.01,\n",
        "                 value_coefficent=0.5, optimizer=None):\n",
        "        self.env = env\n",
        "        self.net = net\n",
        "        self.clip_ratio = clip_ratio\n",
        "        self.gamma = gamma\n",
        "        self.entropy_coefficent = entropy_coefficent\n",
        "        self.value_coefficent = value_coefficent\n",
        "        self.optimizer = optimizer or tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "        \n",
        "    def train(self, episodes, batch_size):\n",
        "        # Collect experience\n",
        "        states = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        next_states = []\n",
        "        dones = []\n",
        "        episode_reward = 0\n",
        "        state = self.env.reset()\n",
        "        for episode in range(episodes):\n",
        "            for step in range(batch_size):\n",
        "                action_probs, value = self.net(tf.constant([state]))\n",
        "                action = np.random.choice(2, p=action_probs[0])\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                states.append(state)\n",
        "                actions.append(action)\n",
        "                rewards.append(reward)\n",
        "                next_states.append(next_state)\n",
        "                dones.append(done)\n",
        "                episode_reward += reward\n",
        "                state = next_state\n",
        "                if done:\n",
        "                    state = self.env.reset()\n",
        "                    print(\"Episode {}: Reward = {}\".format(episode, episode_reward))\n",
        "                    episode\n"
      ],
      "metadata": {
        "id": "rfvrXFsVh2IC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JtAfgQc9h2KC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "616fg2yph2L3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}